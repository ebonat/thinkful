{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 748 predictions, 150 were misclassified\n"
     ]
    }
   ],
   "source": [
    "#MODEL 1: ORIGINAL LIST (ALREADY ITERATED BASED ON EXAMINATION OF IMDB RATINGS FILE, TRIAL & ERROR)\n",
    "imdb = pd.read_csv('C:\\\\Users\\\\ryan\\\\Desktop\\\\Thinkful DS Sample Data - Main Course\\\\Unit 2\\\\sentiment labelled sentences\\\\imdb_labelled.csv', delimiter = '\\t', header = None)\n",
    "\n",
    "imdb.columns = ['review', 'positive']\n",
    "\n",
    "keywords = ['terrible', 'awful', 'worst', 'bad', 'stupid', 'poor', 'worse', 'attempt', 'crap', 'fail', 'annoying', 'cheap',\n",
    "           'painful', 'avoid', 'slow', 'pretentious', 'problem', 'embarrassing', 'bored', 'horrible', 'lousy', 'unfortunate', \n",
    "           'boring', 'sucks', 'sucked', 'waste', 'unbear', ' mess ', 'wasting', 'mediocre', 'sloppy',\n",
    "           'disappoint', 'garbage', 'whine', 'whiny', 'plot', 'hate ', 'hated', 'negative', 'nobody', 'flaw',\n",
    "           'script', 'insult', 'do not', 'torture', ' lack', 'lame', 'ridiculous', 'not', 'unbelievable', 'skip', 'shame', \n",
    "           'not even', 'miss', 'excellent', 'amazing', 'love', 'incredible', 'fantastic', 'terrific', 'best', 'great', 'fun',\n",
    "           'beautiful', 'well done', 'enjoy', 'perfect', 'smart', 'highly', 'impress', 'well']\n",
    "\n",
    "#removed the required space before/after the keyword to improve model accuracy (many sentences in IMDB dataset began with\n",
    "#these words, so no space in front)\n",
    "for key in keywords:\n",
    "    imdb[str(key)] = imdb.review.str.contains(str(key), case = False)\n",
    "\n",
    "imdb['positive'] = (imdb['positive'] == 1)\n",
    "    \n",
    "data = imdb[keywords]\n",
    "target = imdb['positive']\n",
    "\n",
    "# Our data is binary / boolean, so we're importing the Bernoulli classifier.\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# Instantiate our model and store it in a new variable.\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "# Fit our model to the data.\n",
    "bnb.fit(data, target)\n",
    "\n",
    "pred = bnb.predict(data)\n",
    "\n",
    "print('Out of {} predictions, {} were misclassified'.format(data.shape[0], (pred != target).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model is:  0.799465240642\n",
      "The sensitivity of the model is: 0.927461139896373\n",
      "The specificity of the model is: 0.6629834254143646\n"
     ]
    }
   ],
   "source": [
    "#Test the accuracy, sensitivity, and specificity\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "c = confusion_matrix(target, pred)\n",
    "\n",
    "#Accuracy\n",
    "print('The accuracy of the model is: ', 1-((pred != target).sum()/data.shape[0]))\n",
    "\n",
    "#Sensitivity\n",
    "print('The sensitivity of the model is: {}'.format((c[1][1])/(c[1][1] + c[1][0])))\n",
    "\n",
    "#Specificity\n",
    "print('The specificity of the model is: {}'.format((c[0][0])/(c[0][0] + c[0][1])))\n",
    "\n",
    "#print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model (fold 1) is:  0.72\n",
      "The accuracy of the model (fold 2) is:  0.706827309237\n",
      "The accuracy of the model (fold 3) is:  0.799196787149\n"
     ]
    }
   ],
   "source": [
    "#Now let's see how well the model accuracy stands up to cross-validation. 748 predictions, let's use 3 folds of 249, 249, 250 to cross-validate\n",
    "\n",
    "fold1 = data.loc[:249, :]\n",
    "keep1 = data.drop(data.index[:249])\n",
    "fold2 = data.loc[250:498, :]\n",
    "keep2 = data.drop(data.index[250:498])\n",
    "fold3 = data.loc[499:, :]\n",
    "keep3 = data.drop(data.index[499:])\n",
    "\n",
    "targ_fold1 = target.loc[:249]\n",
    "targ_keep1 = target.drop(target.index[:249])\n",
    "targ_fold2 = target.loc[250:498]\n",
    "targ_keep2 = target.drop(target.index[250:498])\n",
    "targ_fold3 = target.loc[499:]\n",
    "targ_keep3 = target.drop(target.index[499:])\n",
    "\n",
    "bnb.fit(keep1, targ_keep1)\n",
    "pred = bnb.predict(fold1)\n",
    "\n",
    "#Accuracy\n",
    "print('The accuracy of the model (fold 1) is: ', 1-((pred != targ_fold1).sum()/len(pred)))\n",
    "\n",
    "bnb.fit(keep2, targ_keep2)\n",
    "pred = bnb.predict(fold2)\n",
    "\n",
    "#Accuracy\n",
    "print('The accuracy of the model (fold 2) is: ', 1-((pred != targ_fold2).sum()/len(pred)))\n",
    "\n",
    "bnb.fit(keep3, targ_keep3)\n",
    "pred = bnb.predict(fold3)\n",
    "\n",
    "#Accuracy\n",
    "print('The accuracy of the model (fold 3) is: ', 1-((pred != targ_fold3).sum()/len(pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Given the results of the cross-validation, there does appear to be some over-fitting for my initial model. \n",
    "#Accuracies ranged from 70.7% to 79.9% using 3 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model is:  0.578877005348\n",
      "The sensitivity of the model is: 1.0\n",
      "The specificity of the model is: 0.1298342541436464\n"
     ]
    }
   ],
   "source": [
    "#MODEL 2: Try to minimize false positives (minimize the number of reviews tagged as negative that are actually positive)\n",
    "#In this instance, we don't care as much about accuracy as we do about categorizing a negative review incorrectly...\n",
    "\n",
    "keywords = ['awful', 'worst', 'trash', 'painful', 'sloppy', 'pretentious', 'embarrassing', 'hate', 'torture', 'skip']\n",
    "\n",
    "for key in keywords:\n",
    "    imdb[str(key)] = imdb.review.str.contains(str(key), case = False)\n",
    "\n",
    "imdb['positive'] = (imdb['positive'] == 1)\n",
    "    \n",
    "data = imdb[keywords]\n",
    "target = imdb['positive']\n",
    "\n",
    "# Our data is binary / boolean, so we're importing the Bernoulli classifier.\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# Instantiate our model and store it in a new variable.\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "# Fit our model to the data.\n",
    "bnb.fit(data, target)\n",
    "\n",
    "pred = bnb.predict(data)\n",
    "            \n",
    "#Test the accuracy, sensitivity, and specificity\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "c = confusion_matrix(target, pred)\n",
    "\n",
    "#Accuracy\n",
    "print('The accuracy of the model is: ', 1-((pred != target).sum()/data.shape[0]))\n",
    "\n",
    "#Sensitivity\n",
    "print('The sensitivity of the model is: {}'.format((c[1][1])/(c[1][1] + c[1][0])))\n",
    "\n",
    "#Specificity\n",
    "print('The specificity of the model is: {}'.format((c[0][0])/(c[0][0] + c[0][1])))\n",
    "\n",
    "#print(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#MODEL 3: Try to maximize accuracy using positive sentiment wordlist from internet (words from http://ptrckprry.com/course/ssd/data/positive-words.txt):\n",
    "df = pd.read_csv('C:\\\\Users\\\\ryan\\\\Desktop\\\\Thinkful DS Sample Data - Main Course\\\\Unit 2\\\\sentiment labelled sentences\\\\positive_word_list_from_internet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_list = df['positive_sentiment_list'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model is:  0.780748663102\n",
      "The sensitivity of the model is: 0.6321243523316062\n",
      "The specificity of the model is: 0.9392265193370166\n"
     ]
    }
   ],
   "source": [
    "imdb = pd.read_csv('C:\\\\Users\\\\ryan\\\\Desktop\\\\Thinkful DS Sample Data - Main Course\\\\Unit 2\\\\sentiment labelled sentences\\\\imdb_labelled.csv', delimiter = '\\t', header = None)\n",
    "\n",
    "imdb.columns = ['review', 'positive_review']\n",
    "\n",
    "keywords = pos_list\n",
    "\n",
    "for key in keywords:\n",
    "    imdb[str(key)] = imdb.review.str.contains(str(key), case = False)\n",
    "\n",
    "imdb['positive_review'] = (imdb['positive_review'] == 1)\n",
    "    \n",
    "data = imdb[keywords]\n",
    "target = imdb['positive_review']\n",
    "\n",
    "# Our data is binary / boolean, so we're importing the Bernoulli classifier.\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# Instantiate our model and store it in a new variable.\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "# Fit our model to the data.\n",
    "bnb.fit(data, target)\n",
    "\n",
    "pred = bnb.predict(data)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "c = confusion_matrix(target, pred)\n",
    "\n",
    "#Accuracy\n",
    "print('The accuracy of the model is: ', 1-((pred != target).sum()/data.shape[0]))\n",
    "\n",
    "#Sensitivity\n",
    "print('The sensitivity of the model is: {}'.format((c[1][1])/(c[1][1] + c[1][0])))\n",
    "\n",
    "#Specificity\n",
    "print('The specificity of the model is: {}'.format((c[0][0])/(c[0][0] + c[0][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Positive keyword list was slightly less accurate than my model. Sensitivity and specificity were lower and higher, respectively.\n",
    "#What about it's cross-validation performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model (fold 1) is:  0.524\n",
      "The accuracy of the model (fold 2) is:  0.489959839357\n",
      "The accuracy of the model (fold 3) is:  0.4859437751\n"
     ]
    }
   ],
   "source": [
    "#Now let's see how well the model accuracy stands up to cross-validation. 748 predictions, let's use 3 folds of 249, 249, 250 to cross-validate\n",
    "\n",
    "fold1 = data.loc[:249, :]\n",
    "keep1 = data.drop(data.index[:249])\n",
    "fold2 = data.loc[250:498, :]\n",
    "keep2 = data.drop(data.index[250:498])\n",
    "fold3 = data.loc[499:, :]\n",
    "keep3 = data.drop(data.index[499:])\n",
    "\n",
    "targ_fold1 = target.loc[:249]\n",
    "targ_keep1 = target.drop(target.index[:249])\n",
    "targ_fold2 = target.loc[250:498]\n",
    "targ_keep2 = target.drop(target.index[250:498])\n",
    "targ_fold3 = target.loc[499:]\n",
    "targ_keep3 = target.drop(target.index[499:])\n",
    "\n",
    "bnb.fit(keep1, targ_keep1)\n",
    "pred = bnb.predict(fold1)\n",
    "\n",
    "#Accuracy\n",
    "print('The accuracy of the model (fold 1) is: ', 1-((pred != targ_fold1).sum()/len(pred)))\n",
    "\n",
    "bnb.fit(keep2, targ_keep2)\n",
    "pred = bnb.predict(fold2)\n",
    "\n",
    "#Accuracy\n",
    "print('The accuracy of the model (fold 2) is: ', 1-((pred != targ_fold2).sum()/len(pred)))\n",
    "\n",
    "bnb.fit(keep3, targ_keep3)\n",
    "pred = bnb.predict(fold3)\n",
    "\n",
    "#Accuracy\n",
    "print('The accuracy of the model (fold 3) is: ', 1-((pred != targ_fold3).sum()/len(pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Model 3, with only positive keywords, suffered tremendously when performing cross-validation. The data is over-fitting greatly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Model 4: Try to maximize accuracy by using negative sentiment wordlist from internet (words from http://ptrckprry.com/course/ssd/data/negative-words.txt):\n",
    "df2 = pd.read_csv('C:\\\\Users\\\\ryan\\\\Desktop\\\\Thinkful DS Sample Data - Main Course\\\\Unit 2\\\\sentiment labelled sentences\\\\negative_word_list_from_internet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neg_list = df2['negative_keywords_from_internet'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model is:  0.741978609626\n",
      "The sensitivity of the model is: 0.9870466321243523\n",
      "The specificity of the model is: 0.48066298342541436\n"
     ]
    }
   ],
   "source": [
    "imdb = pd.read_csv('C:\\\\Users\\\\ryan\\\\Desktop\\\\Thinkful DS Sample Data - Main Course\\\\Unit 2\\\\sentiment labelled sentences\\\\imdb_labelled.csv', delimiter = '\\t', header = None)\n",
    "\n",
    "imdb.columns = ['review', 'positive']\n",
    "\n",
    "keywords = neg_list\n",
    "\n",
    "## HAD TO REMOVE SPECIAL CHARACTERS (*, -, etc)\n",
    "for key in keywords:\n",
    "    imdb[str(key)] = imdb.review.str.contains(str(key), case = False)\n",
    "    \n",
    "imdb['positive'] = (imdb['positive'] == 1)\n",
    "\n",
    "data = imdb[keywords]\n",
    "target = imdb['positive']\n",
    "\n",
    "# Our data is binary / boolean, so we're importing the Bernoulli classifier.\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# Instantiate our model and store it in a new variable.\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "# Fit our model to the data.\n",
    "bnb.fit(data, target)\n",
    "\n",
    "pred = bnb.predict(data)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "c = confusion_matrix(target, pred)\n",
    "\n",
    "#Accuracy\n",
    "print('The accuracy of the model is: ', 1-((pred != target).sum()/data.shape[0]))\n",
    "\n",
    "#Sensitivity\n",
    "print('The sensitivity of the model is: {}'.format((c[1][1])/(c[1][1] + c[1][0])))\n",
    "\n",
    "#Specificity\n",
    "print('The specificity of the model is: {}'.format((c[0][0])/(c[0][0] + c[0][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model (fold 1) is:  0.476\n",
      "The accuracy of the model (fold 2) is:  0.465863453815\n",
      "The accuracy of the model (fold 3) is:  0.369477911647\n"
     ]
    }
   ],
   "source": [
    "#Now let's see how well the model accuracy stands up to cross-validation. 748 predictions, let's use 3 folds of 249, 249, 250 to cross-validate\n",
    "\n",
    "fold1 = data.loc[:249, :]\n",
    "keep1 = data.drop(data.index[:249])\n",
    "fold2 = data.loc[250:498, :]\n",
    "keep2 = data.drop(data.index[250:498])\n",
    "fold3 = data.loc[499:, :]\n",
    "keep3 = data.drop(data.index[499:])\n",
    "\n",
    "targ_fold1 = target.loc[:249]\n",
    "targ_keep1 = target.drop(target.index[:249])\n",
    "targ_fold2 = target.loc[250:498]\n",
    "targ_keep2 = target.drop(target.index[250:498])\n",
    "targ_fold3 = target.loc[499:]\n",
    "targ_keep3 = target.drop(target.index[499:])\n",
    "\n",
    "bnb.fit(keep1, targ_keep1)\n",
    "pred = bnb.predict(fold1)\n",
    "\n",
    "#Accuracy\n",
    "print('The accuracy of the model (fold 1) is: ', 1-((pred != targ_fold1).sum()/len(pred)))\n",
    "\n",
    "bnb.fit(keep2, targ_keep2)\n",
    "pred = bnb.predict(fold2)\n",
    "\n",
    "#Accuracy\n",
    "print('The accuracy of the model (fold 2) is: ', 1-((pred != targ_fold2).sum()/len(pred)))\n",
    "\n",
    "bnb.fit(keep3, targ_keep3)\n",
    "pred = bnb.predict(fold3)\n",
    "\n",
    "#Accuracy\n",
    "print('The accuracy of the model (fold 3) is: ', 1-((pred != targ_fold3).sum()/len(pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Why do the positive and negative keyword lists suffer so much when performing cross-validation??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#MODEL 5: Positive & Negative sentiment lists combined (from internet, not my list)\n",
    "\n",
    "posneg_list = pos_list + neg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model is:  0.816844919786\n",
      "The sensitivity of the model is: 0.9844559585492227\n",
      "The specificity of the model is: 0.638121546961326\n"
     ]
    }
   ],
   "source": [
    "imdb = pd.read_csv('C:\\\\Users\\\\ryan\\\\Desktop\\\\Thinkful DS Sample Data - Main Course\\\\Unit 2\\\\sentiment labelled sentences\\\\imdb_labelled.csv', delimiter = '\\t', header = None)\n",
    "\n",
    "imdb.columns = ['review', 'positive_review']\n",
    "\n",
    "keywords = posneg_list\n",
    "\n",
    "for key in keywords:\n",
    "    imdb[str(key)] = imdb.review.str.contains(str(key), case = False)\n",
    "\n",
    "imdb['positive_review'] = (imdb['positive_review'] == 1)\n",
    "    \n",
    "data = imdb[keywords]\n",
    "target = imdb['positive_review']\n",
    "\n",
    "# Our data is binary / boolean, so we're importing the Bernoulli classifier.\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# Instantiate our model and store it in a new variable.\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "# Fit our model to the data.\n",
    "bnb.fit(data, target)\n",
    "\n",
    "pred = bnb.predict(data)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "c = confusion_matrix(target, pred)\n",
    "\n",
    "#Accuracy\n",
    "print('The accuracy of the model is: ', 1-((pred != target).sum()/data.shape[0]))\n",
    "\n",
    "#Sensitivity\n",
    "print('The sensitivity of the model is: {}'.format((c[1][1])/(c[1][1] + c[1][0])))\n",
    "\n",
    "#Specificity\n",
    "print('The specificity of the model is: {}'.format((c[0][0])/(c[0][0] + c[0][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The model performs best with both positive and negative keywords, and this model outperformed my original model in terms of \n",
    "#accuracy. Let's see how it does on cross-validation.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model (fold 1) is:  0.46\n",
      "The accuracy of the model (fold 2) is:  0.465863453815\n",
      "The accuracy of the model (fold 3) is:  0.40562248996\n"
     ]
    }
   ],
   "source": [
    "#Now let's see how well the model accuracy stands up to cross-validation. 748 predictions, let's use 3 folds of 249, 249, 250 to cross-validate\n",
    "\n",
    "fold1 = data.loc[:249, :]\n",
    "keep1 = data.drop(data.index[:249])\n",
    "fold2 = data.loc[250:498, :]\n",
    "keep2 = data.drop(data.index[250:498])\n",
    "fold3 = data.loc[499:, :]\n",
    "keep3 = data.drop(data.index[499:])\n",
    "\n",
    "targ_fold1 = target.loc[:249]\n",
    "targ_keep1 = target.drop(target.index[:249])\n",
    "targ_fold2 = target.loc[250:498]\n",
    "targ_keep2 = target.drop(target.index[250:498])\n",
    "targ_fold3 = target.loc[499:]\n",
    "targ_keep3 = target.drop(target.index[499:])\n",
    "\n",
    "bnb.fit(keep1, targ_keep1)\n",
    "pred = bnb.predict(fold1)\n",
    "\n",
    "#Accuracy\n",
    "print('The accuracy of the model (fold 1) is: ', 1-((pred != targ_fold1).sum()/len(pred)))\n",
    "\n",
    "bnb.fit(keep2, targ_keep2)\n",
    "pred = bnb.predict(fold2)\n",
    "\n",
    "#Accuracy\n",
    "print('The accuracy of the model (fold 2) is: ', 1-((pred != targ_fold2).sum()/len(pred)))\n",
    "\n",
    "bnb.fit(keep3, targ_keep3)\n",
    "pred = bnb.predict(fold3)\n",
    "\n",
    "#Accuracy\n",
    "print('The accuracy of the model (fold 3) is: ', 1-((pred != targ_fold3).sum()/len(pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
